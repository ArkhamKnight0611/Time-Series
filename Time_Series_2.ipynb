{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Time-Dependent Seasonal Components:\n",
        "\n",
        "In time series analysis, seasonal components are recurring patterns within a specific time period (e.g., daily, weekly, yearly).\n",
        "Time-dependent seasonal components deviate from the usual seasonal pattern and change over time.\n",
        "This means the magnitude or timing of seasonal fluctuations can vary across different periods within the data.\n",
        "Q2. Identifying Time-Dependent Seasonality:\n",
        "\n",
        "Visual inspection: Plotting the data over time can reveal changes in the seasonal pattern. For instance, ice cream sales might show a stronger seasonal peak in summer months over recent years compared to historical data.\n",
        "Statistical tests: Techniques like segmented regression can identify structural breaks in the data, potentially indicating shifts in seasonality.\n",
        "Time-varying spectral analysis: Advanced methods decompose the data into time-dependent seasonal components.\n",
        "Q3. Factors Influencing Time-Dependent Seasonality:\n",
        "\n",
        "Long-term trends: Overall growth or decline in a time series can impact the seasonal pattern. For example, a growing e-commerce market might dampen seasonal sales fluctuations in brick-and-mortar stores.\n",
        "Changes in consumer behavior: Shifts in preferences or buying patterns can alter seasonal demand.\n",
        "External factors: Economic events, weather fluctuations, or marketing campaigns can introduce temporary variations in seasonality.\n",
        "Q4. Autoregression Models in Time Series Analysis:\n",
        "\n",
        "Autoregression (AR) models use past values of a time series to predict future values.\n",
        "The basic idea is that future values are linearly related to a certain number of past values (lags).\n",
        "AR models are denoted as AR(p), where p represents the number of lags used for prediction.\n",
        "Q5. Making Predictions with Autoregression Models:\n",
        "\n",
        "To predict future values (y_t), an AR(p) model uses a weighted sum of p past values (y_(t-1), y_(t-2), ..., y_(t-p)) along with a constant term (intercept).\n",
        "The weights (coefficients) are estimated from historical data using statistical methods like least squares regression.\n",
        "Q6. Moving Average (MA) Models:\n",
        "\n",
        "MA models use the average of past forecast errors (residuals) to improve predictions.\n",
        "\n",
        "The model assumes that current errors are influenced by a certain number of past errors (q lags).\n",
        "\n",
        "MA models are denoted as MA(q), where q represents the number of past errors considered.\n",
        "\n",
        "Key Difference from AR Models: AR models rely on past values of the actual data, while MA models focus on past forecast errors to adjust predictions.\n",
        "\n",
        "Q7. Mixed ARMA Models:\n",
        "\n",
        "ARMA (Autoregressive Integrated Moving Average) models combine both AR and MA components.\n",
        "An ARMA(p, q) model incorporates p past values of the data and q past forecast errors for prediction.\n",
        "ARMA models are more flexible than pure AR or MA models, allowing for a wider range of time series patterns to be captured."
      ],
      "metadata": {
        "id": "E_PyVOyaTYR9"
      }
    }
  ]
}